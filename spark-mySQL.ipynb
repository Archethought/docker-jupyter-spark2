{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept - Read from MySQL Database\n",
    "\n",
    "## Setup Database Server\n",
    "```\n",
    "docker run --detach --name=mysql-db --env=\"MYSQL_ROOT_PASSWORD=datascience\" mysql\n",
    "```\n",
    "\n",
    "This will use the [official mysql docker image](https://hub.docker.com/_/mysql/) to create a database server container with root password 'datascience'.\n",
    "Capture the IP address of the DB server container with `docker inspect mysql-db | grep -i 'ipaddress'`\n",
    "This will yeild something like\n",
    "```\n",
    "            \"SecondaryIPAddresses\": null,\n",
    "            \"IPAddress\": \"172.17.0.8\",\n",
    "                    \"IPAddress\": \"172.17.0.8\",\n",
    "```\n",
    "\n",
    "## Command Line Client\n",
    "To create a database and add data, access the mySql server through a command line by creating another mysql container linked to the server container. \n",
    "This container will only exist until you exit the mysql command prompt. \n",
    "Assume data you want to load into the database is in /home/cownby/data/\n",
    "Run mysql command line client:\n",
    "```\n",
    "export MYSQL_PORT_3306_TCP_ADDR=\"172.17.0.8:6603\"\n",
    "export MYSQL_ROOT_PASSWORD=\"datascience\"\n",
    "export MYSQL_SERVER_CONTAINER=\"mysql-db\"\n",
    "\n",
    "docker run -it -v \"/home/cownby/data/\":\"/data\" --link \"$MYSQL_SERVER_CONTAINER\":mysql --rm mysql sh -c 'exec mysql -h\"$MYSQL_PORT_3306_TCP_ADDR\" -P\"$MYSQL_PORT_3306_TCP_PORT\" -uroot -p\"$MYSQL_ROOT_PASSWORD\"'\n",
    "\n",
    "```\n",
    "You will be prompted again for the password\n",
    "\n",
    "### SQL Commands\n",
    "\n",
    "```SQL\n",
    "/*  \n",
    "Assume volume is mounted to `/data` and data is in file `supers.csv`.\n",
    "\n",
    "sample data:\n",
    "id,firstName,lastName,superpower,goodDeeds,maxSpeed\n",
    "0,100,carolyn,ownby,smile,72,15.5\n",
    "0,200,krizia,conrad,good food!,80,25.7\n",
    "0,300,jordan,dick,roadster,30,80.25\n",
    "0,400,dixon,dick,vision,65,56.8\n",
    "\n",
    "*/\n",
    "\n",
    "create database giskard;\n",
    "use giskard;\n",
    "create table supers (\n",
    "\tkey_id      int NOT NULL AUTO_INCREMENT UNIQUE,\n",
    "\timport_id   int,\n",
    "\tfirst_name  varchar(20),\n",
    "\tlast_name   varchar(20),\n",
    "\tsuper_power varchar(20),\n",
    "\tgood_deeds  int,\n",
    "\tmax_speed   float\n",
    ");\n",
    "\n",
    "load data local infile '/data/supers.csv'\n",
    "      into table supers\n",
    "      fields terminated by ','\n",
    "      lines terminated by '\\n'\n",
    "      ignore 1 rows;\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "* [Excel to MySQL](http://www.prcconsulting.net/2016/10/migrating-an-excel-spreadsheet-to-mysql-and-to-spark-2-0-1-part-1/)  \n",
    "This is NOT done programmatically, but by simply exporting an excel sheet as csv and importing into mySQL.\n",
    "* [Spark & MySQL](http://www.prcconsulting.net/wp-content/uploads/2016/10/Connect_MySQL.py_-1.html)\n",
    "* [Spark & database properties](http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases)\n",
    "* [tutorial](http://severalnines.com/blog/mysql-docker-containers-understanding-basics)\n",
    "* [official mysql docker repository](https://hub.docker.com/_/mysql/)\n",
    "\n",
    "\n",
    "##  Test Notes\n",
    "Create new container linked to mysql server\n",
    "\n",
    "```bash\n",
    "sudo docker run -d -p 8826:8888  -e GRANT_SUDO=yes --user root --net=hadoop --name=spark-mysql --link mysql-db:spark-mysql  cotest2-image  start-notebook.sh \n",
    "\n",
    "docker run -it -v \"/home/ubuntu/carolyn/data\":\"/data\" --link mysql-db:mysql --rm mysql sh -c 'exec mysql -h\"$MYSQL_PORT_3306_TCP_ADDR\" -P\"$MYSQL_PORT_3306_TCP_PORT\" -uroot -p\"$MYSQL_ROOT_PASSWORD\"' \n",
    "\n",
    "\n",
    "\n",
    " #local VM:\n",
    " docker run -d -p 8826:8888  -e GRANT_SUDO=yes --user root --net=hadoop --name=cotest cotest-image  start-notebook.sh\n",
    "\n",
    " \n",
    " time docker build -t cotest-image . 1>build.log 2>build.err  ;ll\n",
    " \n",
    " docker run -d -p 8826:8888  -e GRANT_SUDO=yes --user root --net=hadoop --name=cotest-mysql --link mysql-db:cotest-mysql  cotest-image  start-notebook.sh\n",
    " \n",
    " docker stop cotest-mysql;docker rm cotest-mysql;docker rmi cotest-image\n",
    " \n",
    " find / -name *.jar 2>&1 | grep -v \"Permission denied\" | grep -i mysql\n",
    " root@96e60e2c77f8:~/work# find / -name *.jar 2>&1 | grep -v \"Permission denied\" | grep -i mysql      \n",
    "/usr/share/maven-repo/mysql/mysql-connector-java/5.1.39/mysql-connector-java-5.1.39.jar\n",
    "/usr/share/maven-repo/mysql/mysql-connector-java/debian/mysql-connector-java-debian.jar\n",
    "/usr/share/java/mysql.jar\n",
    "/usr/share/java/mysql-connector-java-5.1.39.jar\n",
    "/usr/share/java/mysql-connector-java.jar\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic setup required in all notebooks\n",
    " \n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Define an arbitrary application name & stand-alone Spark cluster\n",
    "appName='archetest'\n",
    "master='local[*]' #local spark-master\n",
    "\n",
    "# Explicitly define python 2 since we have both versions 2 & 3 installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/envs/python2/bin/python'\n",
    "\n",
    "# Create spark context with which we will reference the Spark API\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(master)\n",
    "         .config(conf=SparkConf())  \n",
    "         .appName(appName)\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#conf.setJars()\n",
    "#.setjars(Array(\"/usr/local/spark-2.0.0-bin-hadoop2.7/jars\"))\n",
    "\n",
    "sys.version\n",
    "#example\n",
    "#3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
    "\n",
    "#/usr/share/java/mysql-connector-java.jar\n",
    "import sys,os,os.path\n",
    "sys.path.append('/usr/share/java')\n",
    "!echo $PATH\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  \n",
    "!sudo apt-get update && apt-get install -y --no-install-recommends apt-utils\n",
    "!sudo apt-get install -y mysql-client\n",
    "!sudo apt-get install -y python-dev \n",
    "!sudo apt-get install -y libmysqlclient-dev\n",
    "!sudo apt-get install -y libmysql-java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip2 install MySQL-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:49)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createConnectionFactory(JdbcUtils.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:123)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:117)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:345)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:122)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-82569d5a490e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'root'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'datascience'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         dbtable='supers',driver='com.mysql.jdbc.Driver')\n\u001b[0m\u001b[0;32m     15\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m      )\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:49)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createConnectionFactory(JdbcUtils.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:123)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:117)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:345)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:122)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "#URL = 'jdbc:mysql://172.17.0.8:6603/giskard'\n",
    "#URL = 'jdbc:mysql://172.17.0.8:6603/giskard?user=root&password=datascience'\n",
    "URL = 'jdbc:mysql://172.17.0.8:6603/giskard'\n",
    "\n",
    "#spark.conf.set(spark.sparkContext.environment.\n",
    "#    spark.driver.extraClassPath, r'/usr/share/java/mysql-connector-java.jar')\n",
    "#spark.executor.extraClassPath = r'/usr/share/java/mysql-connector-java.jar'\n",
    "\n",
    "df = (spark.read.format('jdbc')\n",
    "      .options(\n",
    "        url=URL,\n",
    "        user='root',\n",
    "        password='datascience',\n",
    "        dbtable='supers',driver='com.mysql.jdbc.Driver')\n",
    "      .load()\n",
    "     )\n",
    "\n",
    "# driver='com.mysql.jdbc.Driver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APACHE_SPARK_VERSION': '2.0.2',\n",
       " 'CLICOLOR': '1',\n",
       " 'CONDA_DIR': '/opt/conda',\n",
       " 'DEBIAN_FRONTEND': 'noninteractive',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'GRANT_SUDO': 'yes',\n",
       " 'HOME': '/home/jovyan',\n",
       " 'HOSTNAME': 'e004b18c30ec',\n",
       " 'JPY_PARENT_PID': '9',\n",
       " 'LANG': 'en_US.UTF-8',\n",
       " 'LANGUAGE': 'en_US.UTF-8',\n",
       " 'LC_ALL': 'en_US.UTF-8',\n",
       " 'LOGNAME': 'jovyan',\n",
       " 'MAIL': '/var/mail/jovyan',\n",
       " 'MESOS_NATIVE_LIBRARY': '/usr/local/lib/libmesos.so',\n",
       " 'NB_UID': '1000',\n",
       " 'NB_USER': 'jovyan',\n",
       " 'PAGER': 'cat',\n",
       " 'PATH': '/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       " 'PWD': '/home/jovyan/work',\n",
       " 'PYSPARK_PYTHON': '/opt/conda/envs/python2/bin/python',\n",
       " 'PYTHONPATH': '/usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.4-src.zip:/usr/lib/python2.7/dist-packages',\n",
       " 'R_LIBS_USER': '/usr/local/spark/R/lib',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'SPARK_HOME': '/usr/local/spark',\n",
       " 'SPARK_OPTS': '--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info --spark-jars=/usr/share/java/mysql-connector-java.jars',\n",
       " 'TERM': 'xterm-color',\n",
       " 'USER': 'jovyan',\n",
       " '_': '/usr/bin/env'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
